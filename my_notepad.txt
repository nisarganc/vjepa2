

Training @helma-apptainer:

apptainer shell vjepa2.sif
. /opt/conda/etc/profile.d/conda.sh
conda activate vjepa-312


1. Fix def file to add git creds to download simple_move
2. add to requirement file: lerobot @ git+https://github.com/huggingface/lerobot@0cf864870cf29f4738d3ade893e6fd13fbd7cdb5

- - - - - - - - - - - - - - - - - - - 

Dataset inference on AIR-Tower:

Droid:
    1. export TF_FORCE_GPU_ALLOW_GROWTH=true
    2. load full episode for inference using droid dataset loader

UTN:
    1. check cem params
    2. check compute new pose against next states
    2. compare action cordinates and gripper conventions with droid

- - - - - - - - - - - - - - - - - - - 

Run robot inference on AIR-Tower


1. Fix cam port in vjepa2_ac_robotinference.py

2. check doid EE coordinates and gripper offset 
    - check if implementation of ee state and gripper state matches with it

3. Fix observation reading in lab from cameras (same pos as droid, resolution, etc)
    - resolution: 256  x 256
    - frame-rate: 4 fps

4. Run pretrained v-jepa-ac action inference until goal_image state is reached
    - closed-loop model-predictive control

- - - - - - - - - - - - - - - - - - - 

Next steps:
1. Try key frame prediction with gemini: nano-banana
2. Maybe finetune vjepa-ac on Droid with 16 fps (keep constant action chunking)
2. Action chunking with 10 skipsteps
3. Checkout NovaFlow paper 
4. Title: zero shot manipulation with video and world models

Tests:
1. Check how sensitive the ac predictor is to the sub_goal_images
2. Check also for human hand prediction?

BootCamp:
1. Try bi-manual dataset from Bootcamp
2. Download slides

- - - - - - - - - - - - - - - - - - - 
